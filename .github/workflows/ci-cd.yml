name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

jobs:
  cleanup-logs:
    name: Initialize Log Directory
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: Initialize logs directory
        run: |
          mkdir -p logs
          echo "# CI/CD Log Directory" > logs/README.md
          echo "Created $(date)" >> logs/README.md
          echo "This directory contains logs from the CI/CD pipeline." >> logs/README.md
          
  lint:
    name: Lint Code
    runs-on: ubuntu-latest
    needs: [cleanup-logs]
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install flake8 mypy black isort
      
      - name: Run flake8
        run: flake8 src/ tests/
      
      - name: Run mypy
        run: mypy src/
      
      - name: Run isort
        run: isort --check-only --profile black src/ tests/
      
      - name: Run black
        run: black --check src/ tests/

  test-python:
    name: Python Tests
    runs-on: ubuntu-latest
    needs: [lint]
    services:
      postgres:
        image: postgres:14-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: ultimatemarketing
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:6.2-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      rabbitmq:
        image: rabbitmq:3.9-alpine
        ports:
          - 5672:5672
        env:
          RABBITMQ_DEFAULT_USER: guest
          RABBITMQ_DEFAULT_PASS: guest
        options: >-
          --health-cmd "rabbitmqctl status"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov
      
      - name: Set up database schema
        run: |
          export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/ultimatemarketing
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          psql -h localhost -U postgres -c "CREATE SCHEMA IF NOT EXISTS umt" ultimatemarketing
        env:
          PGPASSWORD: postgres
      
      - name: Run migrations
        run: |
          export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/ultimatemarketing
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          alembic upgrade head
      
      - name: Run tests
        run: |
          export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/ultimatemarketing
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          # Create token directory for SEO tests
          mkdir -p .tokens
          # Run tests with output capture to file for analysis
          pytest -xvs tests/ --cov=src --cov-report=xml --cov-report=term | tee test-output.txt
        env:
          REDIS_URL: redis://localhost:6379/0
          RABBITMQ_URL: amqp://guest:guest@localhost:5672/%2F
          # Mock values for SEO tests to ensure they're skipped when credentials aren't available
          GOOGLE_OAUTH2_CLIENT_ID: "mock_client_id_for_ci"
          GOOGLE_OAUTH2_CLIENT_SECRET: "mock_client_secret_for_ci"
          TOKEN_STORAGE_DIR: ".tokens"
      
      - name: Check test coverage
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          coverage report --fail-under=80
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
      
      - name: Analyze test skips
        if: always()
        run: |
          echo "# Test Skip Analysis" > test-skips.md
          echo "## Date: $(date)" >> test-skips.md
          echo "### Skipped Tests:" >> test-skips.md
          if [ -f test-output.txt ]; then
            grep -A 1 "SKIPPED" test-output.txt >> test-skips.md || echo "No skipped tests found." >> test-skips.md
          else
            echo "No test output file found." >> test-skips.md
          fi
      
      - name: Upload test skip analysis
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-skips-report
          path: test-skips.md
      
      - name: Collect log files
        if: always()
        run: |
          mkdir -p collected-logs/python-tests
          if [ -d "logs" ]; then
            cp logs/*.log* collected-logs/python-tests/ 2>/dev/null || true
          fi
          echo "## Python Test Logs - $(date)" > collected-logs/python-tests/README.md
          echo "These logs were collected from the Python Tests job." >> collected-logs/python-tests/README.md
          ls -la collected-logs/python-tests/ >> collected-logs/python-tests/README.md
      
      - name: Upload log files
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: python-test-logs
          path: collected-logs/python-tests/
          if-no-files-found: ignore
          retention-days: 7

  test-js:
    name: Frontend Tests
    runs-on: ubuntu-latest
    needs: [cleanup-logs, lint]
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 16
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      
      - name: Install dependencies
        run: cd frontend && npm ci
      
      - name: Run tests
        run: cd frontend && npm test

  verify-migrations:
    name: Verify Migration Files
    runs-on: ubuntu-latest
    needs: [cleanup-logs, lint]
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Verify migration patterns
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python scripts/check_migration_patterns.py
      
      - name: Run migration verification
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python manage_migrations.py verify
      
      - name: Run verification tests
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python tests/scripts/run_verification_tests.py --output verification-test-results.xml
      
      - name: Upload verification results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: migration-verification-results
          path: |
            migration_validation.log
            verification-test-results.xml
          if-no-files-found: ignore
          
      - name: Collect migration verification logs
        if: always()
        run: |
          mkdir -p collected-logs/migration-verification
          if [ -d "logs" ]; then
            cp logs/*.log* collected-logs/migration-verification/ 2>/dev/null || true
          fi
          echo "## Migration Verification Logs - $(date)" > collected-logs/migration-verification/README.md
          echo "These logs were collected from the Migration Verification job." >> collected-logs/migration-verification/README.md
          ls -la collected-logs/migration-verification/ >> collected-logs/migration-verification/README.md
      
      - name: Upload migration verification logs
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: migration-verification-logs
          path: collected-logs/migration-verification/
          if-no-files-found: ignore
          retention-days: 7
      
      - name: Notify on verification failure
        if: failure()
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python scripts/notify_migration_status.py \
            --environment "${{ github.ref == 'refs/heads/main' && 'production' || 'staging' }}" \
            --status failure \
            --title "Migration Verification Failed" \
            --message "Migration verification failed in the CI/CD pipeline. Please check the logs to fix the issues." \
            --slack-webhook "${{ secrets.SLACK_WEBHOOK }}" \
            --email-recipients "${{ secrets.MIGRATION_ALERT_EMAILS }}"

  test-migrations:
    name: Test Migrations
    runs-on: ubuntu-latest
    needs: [verify-migrations]
    services:
      postgres:
        image: postgres:14-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: ultimatemarketing
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest
      
      - name: Set up database schema
        run: |
          export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/ultimatemarketing
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          psql -h localhost -U postgres -c "CREATE SCHEMA IF NOT EXISTS umt" ultimatemarketing
        env:
          PGPASSWORD: postgres
      
      - name: Run pre-migration checks
        run: |
          export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/ultimatemarketing
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          export CI=true
          python scripts/pre_migration_check.py --skip-simulation
      
      - name: Run migration tests
        run: |
          export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/ultimatemarketing
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          pytest -xvs tests/integration/test_migrations.py
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: ultimatemarketing
          
      - name: Collect migration test logs
        if: always()
        run: |
          mkdir -p collected-logs/migration-tests
          if [ -d "logs" ]; then
            cp logs/*.log* collected-logs/migration-tests/ 2>/dev/null || true
          fi
          echo "## Migration Test Logs - $(date)" > collected-logs/migration-tests/README.md
          echo "These logs were collected from the Migration Tests job." >> collected-logs/migration-tests/README.md
          ls -la collected-logs/migration-tests/ >> collected-logs/migration-tests/README.md
      
      - name: Upload migration test logs
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: migration-test-logs
          path: collected-logs/migration-tests/
          if-no-files-found: ignore
          retention-days: 7

  docker-build-test:
    name: Docker Build Test
    runs-on: ubuntu-latest
    needs: [test-python, test-js, test-migrations, verify-migrations]
    if: github.event_name == 'pull_request'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Build API Gateway
        uses: docker/build-push-action@v4
        with:
          context: .
          file: docker/api_gateway/Dockerfile
          push: false
          load: true
          tags: umt-api-gateway:test
      
      - name: Build Frontend
        uses: docker/build-push-action@v4
        with:
          context: .
          file: docker/frontend/Dockerfile
          push: false
          load: true
          tags: umt-frontend:test
      
      - name: Build Agents
        uses: docker/build-push-action@v4
        with:
          context: .
          file: docker/agents/Dockerfile
          push: false
          load: true
          tags: umt-agents:test
      
      - name: Build Migrations
        uses: docker/build-push-action@v4
        with:
          context: .
          file: docker/migrations/Dockerfile
          push: false
          load: true
          tags: umt-migrations:test
      
      - name: Test Docker Compose
        run: |
          docker-compose -f docker-compose.test.yml up -d
          sleep 10
          docker-compose -f docker-compose.test.yml ps
          docker-compose -f docker-compose.test.yml logs migrations
          
          # Install requests for health check
          pip install requests
          
          # Run health check
          python scripts/check_api_health.py --url http://localhost:8000/api/health --retries 5 --delay 3
          
          # Shutdown containers
          docker-compose -f docker-compose.test.yml down

  build-and-push-ecr:
    name: Build and Push to ECR
    runs-on: ubuntu-latest
    needs: [test-python, test-js, test-migrations, verify-migrations]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    
    outputs:
      app_version: ${{ steps.version.outputs.version }}
      
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Fetch all history for version calculation
      
      - name: Generate Version
        id: version
        run: |
          # Generate semver version based on git commits and branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            BRANCH="prod"
          else
            BRANCH="dev"
          fi
          
          # Get latest tag or use 0.0.0 if none exists
          LATEST_TAG=$(git describe --tags --abbrev=0 2>/dev/null || echo "0.0.0")
          
          # Count commits since tag
          COMMIT_COUNT=$(git rev-list --count ${LATEST_TAG}..HEAD)
          
          # Get short commit hash
          COMMIT_HASH=$(git rev-parse --short HEAD)
          
          # Build version string: {tag}+{commit_count}.{branch}.{commit_hash}
          # Example: 1.2.3+42.dev.a1b2c3d
          VERSION="${LATEST_TAG}+${COMMIT_COUNT}.${BRANCH}.${COMMIT_HASH}"
          
          echo "Generated version: $VERSION"
          echo "version=$VERSION" >> $GITHUB_OUTPUT
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ github.ref == 'refs/heads/main' && 'us-west-2' || 'us-east-1' }}
      
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1
      
      - name: Set environment variables
        run: |
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "ECR_REPOSITORY=${{ secrets.ECR_URI }}" >> $GITHUB_ENV
            echo "S3_BUCKET=${{ secrets.S3_BUCKET }}" >> $GITHUB_ENV
          else
            echo "ECR_REPOSITORY=${{ secrets.ECR_URI_STAGING }}" >> $GITHUB_ENV
            echo "S3_BUCKET=${{ secrets.S3_BUCKET_STAGING }}" >> $GITHUB_ENV
          fi
      
      - name: Build, tag, and push API Gateway image to Amazon ECR
        uses: docker/build-push-action@v4
        with:
          context: .
          file: docker/api_gateway/Dockerfile
          push: true
          tags: |
            ${{ env.ECR_REPOSITORY }}/api-gateway:${{ steps.version.outputs.version }}
            ${{ env.ECR_REPOSITORY }}/api-gateway:latest
      
      - name: Build, tag, and push Frontend image to Amazon ECR
        uses: docker/build-push-action@v4
        with:
          context: .
          file: docker/frontend/Dockerfile
          push: true
          tags: |
            ${{ env.ECR_REPOSITORY }}/frontend:${{ steps.version.outputs.version }}
            ${{ env.ECR_REPOSITORY }}/frontend:latest
      
      - name: Build, tag, and push Agents image to Amazon ECR
        uses: docker/build-push-action@v4
        with:
          context: .
          file: docker/agents/Dockerfile
          push: true
          tags: |
            ${{ env.ECR_REPOSITORY }}/agents:${{ steps.version.outputs.version }}
            ${{ env.ECR_REPOSITORY }}/agents:latest
      
      - name: Build, tag, and push Migrations image to Amazon ECR
        uses: docker/build-push-action@v4
        with:
          context: .
          file: docker/migrations/Dockerfile
          push: true
          tags: |
            ${{ env.ECR_REPOSITORY }}/migrations:${{ steps.version.outputs.version }}
            ${{ env.ECR_REPOSITORY }}/migrations:latest

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [build-and-push-ecr]
    if: github.ref == 'refs/heads/develop' && github.event_name != 'pull_request'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install boto3 requests
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1
      
      - name: Create .env.staging file
        run: |
          cat > .env.staging << EOL
          # Generated by CI/CD pipeline
          DATABASE_URL=${{ secrets.DATABASE_URL_STAGING }}
          POSTGRES_USER=${{ secrets.STAGING_DB_USER }}
          POSTGRES_PASSWORD=${{ secrets.STAGING_DB_PASSWORD }}
          POSTGRES_DB=${{ secrets.STAGING_DB_NAME }}
          POSTGRES_HOST=${{ secrets.STAGING_DB_HOST }}
          POSTGRES_PORT=5432
          RABBITMQ_USER=${{ secrets.STAGING_RABBITMQ_USER }}
          RABBITMQ_PASSWORD=${{ secrets.STAGING_RABBITMQ_PASSWORD }}
          RABBITMQ_URL=amqp://${{ secrets.STAGING_RABBITMQ_USER }}:${{ secrets.STAGING_RABBITMQ_PASSWORD }}@${{ secrets.STAGING_RABBITMQ_HOST }}:5672/%2F
          REDIS_URL=${{ secrets.REDIS_URL_STAGING }}
          VECTOR_DB_USER=${{ secrets.STAGING_VECTOR_DB_USER }}
          VECTOR_DB_PASSWORD=${{ secrets.STAGING_VECTOR_DB_PASSWORD }}
          VECTOR_DB_NAME=${{ secrets.STAGING_VECTOR_DB_NAME }}
          S3_BACKUP_BUCKET=${{ secrets.S3_BUCKET_STAGING }}
          APP_VERSION=${{ needs.build-and-push-ecr.outputs.app_version }}
          ECR_REPOSITORY=${{ secrets.ECR_URI_STAGING }}
          ENVIRONMENT=staging
          EOL
      
      - name: Backup Database
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python scripts/backup_database.py \
            --db-name ${{ secrets.STAGING_DB_NAME }} \
            --db-user ${{ secrets.STAGING_DB_USER }} \
            --db-password ${{ secrets.STAGING_DB_PASSWORD }} \
            --db-host ${{ secrets.STAGING_DB_HOST }} \
            --db-port 5432 \
            --environment staging \
            --upload-to-s3 \
            --s3-bucket ${{ secrets.S3_BUCKET_STAGING }}
        continue-on-error: true  # Don't fail the whole workflow if backup fails
      
      - name: Run Database Migrations
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python scripts/deploy.py \
            --environment staging \
            --db-only
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL_STAGING }}
          POSTGRES_USER: ${{ secrets.STAGING_DB_USER }}
          POSTGRES_PASSWORD: ${{ secrets.STAGING_DB_PASSWORD }}
          POSTGRES_DB: ${{ secrets.STAGING_DB_NAME }}
          POSTGRES_HOST: ${{ secrets.STAGING_DB_HOST }}
          POSTGRES_PORT: 5432
      
      - name: Deploy Services
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python scripts/deploy.py \
            --environment staging \
            --skip-migrations \
            --skip-backup
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL_STAGING }}
          REDIS_URL: ${{ secrets.REDIS_URL_STAGING }}
          APP_VERSION: ${{ needs.build-and-push-ecr.outputs.app_version }}
          ECR_REPOSITORY: ${{ secrets.ECR_URI_STAGING }}
      
      - name: Notify Slack on Success
        if: success()
        run: |
          python scripts/notify_deploy_status.py \
            --environment staging \
            --status success \
            --title "Staging Deployment Successful" \
            --message "Version ${{ needs.build-and-push-ecr.outputs.app_version }} deployed to staging environment" \
            --slack-webhook ${{ secrets.SLACK_WEBHOOK }}
      
      - name: Notify Slack on Failure
        if: failure()
        run: |
          python scripts/notify_deploy_status.py \
            --environment staging \
            --status failure \
            --title "Staging Deployment Failed" \
            --message "Deployment of version ${{ needs.build-and-push-ecr.outputs.app_version }} to staging environment failed" \
            --slack-webhook ${{ secrets.SLACK_WEBHOOK }}

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [build-and-push-ecr]
    if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request'
    environment: production  # Requires manual approval in GitHub
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install boto3 requests
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2
      
      - name: Create .env.production file
        run: |
          cat > .env.production << EOL
          # Generated by CI/CD pipeline
          DATABASE_URL=postgresql://${{ secrets.PROD_DB_USER }}:${{ secrets.PROD_DB_PASSWORD }}@${{ secrets.PROD_DB_HOST }}:5432/${{ secrets.PROD_DB_NAME }}
          POSTGRES_USER=${{ secrets.PROD_DB_USER }}
          POSTGRES_PASSWORD=${{ secrets.PROD_DB_PASSWORD }}
          POSTGRES_DB=${{ secrets.PROD_DB_NAME }}
          POSTGRES_HOST=${{ secrets.PROD_DB_HOST }}
          POSTGRES_PORT=5432
          RABBITMQ_USER=${{ secrets.PROD_RABBITMQ_USER }}
          RABBITMQ_PASSWORD=${{ secrets.PROD_RABBITMQ_PASSWORD }}
          RABBITMQ_URL=amqp://${{ secrets.PROD_RABBITMQ_USER }}:${{ secrets.PROD_RABBITMQ_PASSWORD }}@${{ secrets.PROD_RABBITMQ_HOST }}:5672/%2F
          REDIS_URL=redis://${{ secrets.PROD_REDIS_HOST }}:6379/0
          VECTOR_DB_USER=${{ secrets.PROD_VECTOR_DB_USER }}
          VECTOR_DB_PASSWORD=${{ secrets.PROD_VECTOR_DB_PASSWORD }}
          VECTOR_DB_NAME=${{ secrets.PROD_VECTOR_DB_NAME }}
          S3_BACKUP_BUCKET=${{ secrets.S3_BUCKET }}
          APP_VERSION=${{ needs.build-and-push-ecr.outputs.app_version }}
          ECR_REPOSITORY=${{ secrets.ECR_URI }}
          ENVIRONMENT=production
          EOL
      
      - name: Notify Team of Pending Deployment
        run: |
          python scripts/notify_deploy_status.py \
            --environment production \
            --status info \
            --title "Production Deployment Starting" \
            --message "Starting deployment of version ${{ needs.build-and-push-ecr.outputs.app_version }} to production environment" \
            --slack-webhook ${{ secrets.SLACK_WEBHOOK }} \
            --email-recipients ${{ secrets.OPS_EMAIL }}
      
      - name: Backup Database
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python scripts/backup_database.py \
            --db-name ${{ secrets.PROD_DB_NAME }} \
            --db-user ${{ secrets.PROD_DB_USER }} \
            --db-password ${{ secrets.PROD_DB_PASSWORD }} \
            --db-host ${{ secrets.PROD_DB_HOST }} \
            --db-port 5432 \
            --environment production \
            --upload-to-s3 \
            --s3-bucket ${{ secrets.S3_BUCKET }}
      
      - name: Run Database Migrations
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python scripts/deploy.py \
            --environment production \
            --db-only
        env:
          DATABASE_URL: postgresql://${{ secrets.PROD_DB_USER }}:${{ secrets.PROD_DB_PASSWORD }}@${{ secrets.PROD_DB_HOST }}:5432/${{ secrets.PROD_DB_NAME }}
          POSTGRES_USER: ${{ secrets.PROD_DB_USER }}
          POSTGRES_PASSWORD: ${{ secrets.PROD_DB_PASSWORD }}
          POSTGRES_DB: ${{ secrets.PROD_DB_NAME }}
          POSTGRES_HOST: ${{ secrets.PROD_DB_HOST }}
          POSTGRES_PORT: 5432
      
      - name: Deploy Services
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python scripts/deploy.py \
            --environment production \
            --skip-migrations \
            --skip-backup
        env:
          DATABASE_URL: postgresql://${{ secrets.PROD_DB_USER }}:${{ secrets.PROD_DB_PASSWORD }}@${{ secrets.PROD_DB_HOST }}:5432/${{ secrets.PROD_DB_NAME }}
          APP_VERSION: ${{ needs.build-and-push-ecr.outputs.app_version }}
          ECR_REPOSITORY: ${{ secrets.ECR_URI }}
      
      - name: Notify Slack on Success
        if: success()
        run: |
          python scripts/notify_deploy_status.py \
            --environment production \
            --status success \
            --title "Production Deployment Successful" \
            --message "Version ${{ needs.build-and-push-ecr.outputs.app_version }} deployed to production environment" \
            --slack-webhook ${{ secrets.SLACK_WEBHOOK }} \
            --email-recipients ${{ secrets.OPS_EMAIL }}
      
      - name: Notify Slack on Failure
        if: failure()
        run: |
          python scripts/notify_deploy_status.py \
            --environment production \
            --status failure \
            --title "URGENT: Production Deployment Failed" \
            --message "Deployment of version ${{ needs.build-and-push-ecr.outputs.app_version }} to production environment failed. Please check logs immediately." \
            --slack-webhook ${{ secrets.SLACK_WEBHOOK }} \
            --email-recipients ${{ secrets.OPS_EMAIL }}